{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lê o conteúdo de um arquivo PDF e retorna o texto extraído.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Caminho para o arquivo PDF.\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto completo extraído do PDF.\n",
    "    \"\"\"\n",
    "    print(\"Lendo o arquivo PDF...\")\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    print(\"PDF lido com sucesso.\\n\")\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Divide o texto em chunks (partes menores) com sobreposição para preservar o contexto.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto completo extraído do PDF.\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: Lista de chunks representados como documentos.\n",
    "    \"\"\"\n",
    "    print(\"Dividindo o texto em chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "    )\n",
    "    document = Document(page_content=text, metadata={\"source\": \"PDF Input\"})\n",
    "    chunks = text_splitter.split_documents([document])\n",
    "    print(f\"Texto dividido em {len(chunks)} chunks.\\n\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks: List[Document], openai_api_key: str) -> Chroma:\n",
    "    \"\"\"\n",
    "    Cria um banco de vetores para armazenar os embeddings gerados a partir dos chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Lista de chunks do texto.\n",
    "        openai_api_key: Chave da API da OpenAI para geração de embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        Chroma: Banco de dados vetorial criado.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"db\")\n",
    "    print(\"Banco de vetores criado com sucesso.\\n\")\n",
    "    return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt() -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Configura o prompt para geração de respostas baseadas no contexto recuperado.\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: Template de prompt configurado.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Answer the user query based on context. If you don't know the answer \n",
    "    or the context does not have the answer, say that you don't know.\n",
    "    ALWAYS answer in pt-BR. \n",
    "    Provide the source of your knowledge. I want the answer with a link or document reference.\n",
    "\n",
    "    ## CONTEXT\n",
    "    {contexto}\n",
    "\n",
    "    ## USER QUERY\n",
    "    {pergunta}\n",
    "    \"\"\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(db: Chroma, user_query: str, openai_api_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Pipeline de RAG que recupera informações relevantes e gera uma resposta.\n",
    "    \n",
    "    Args:\n",
    "        db: Banco de vetores criado.\n",
    "        user_query: Pergunta do usuário.\n",
    "        openai_api_key: Chave da API da OpenAI para geração de respostas.\n",
    "    \n",
    "    Returns:\n",
    "        str: Resposta gerada com base no contexto recuperado.\n",
    "    \"\"\"\n",
    "    print(\"Buscando informações relevantes no banco de vetores...\")\n",
    "    context = db.similarity_search_with_relevance_scores(user_query, k=3)\n",
    "    context = [c for c, score in context if score >= 0.7]\n",
    "\n",
    "    if not context:\n",
    "        return \"Eu não sou capaz de responder a essa pergunta.\"\n",
    "\n",
    "    context_text = \"\\n\\n\".join([f\"## Documento {i+1}\\n{chunk.page_content}\" for i, chunk in enumerate(context)])\n",
    "    \n",
    "    print(\"Gerando resposta com base no contexto...\\n\")\n",
    "    prompt = setup_prompt()\n",
    "    chain = (prompt |\n",
    "             ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=openai_api_key) |\n",
    "             StrOutputParser())\n",
    "    \n",
    "    print(\"Resposta:\\n\")\n",
    "    \n",
    "    return chain.invoke({\"contexto\": context_text, \"pergunta\": user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo PDF...\n",
      "PDF lido com sucesso.\n",
      "\n",
      "Dividindo o texto em chunks...\n",
      "Texto dividido em 9 chunks.\n",
      "\n",
      "Banco de vetores criado com sucesso.\n",
      "\n",
      "Buscando informações relevantes no banco de vetores...\n",
      "Gerando resposta com base no contexto...\n",
      "\n",
      "Resposta:\n",
      "\n",
      "A técnica utilizada para a otimização dos hiperparâmetros foi a otimização bayesiana, utilizando a ferramenta Optuna. \n",
      "\n",
      "Fonte: Documento 1, 2 e 3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "file_path = \"/root/pessoal/rag-techniques/naive-rag/Relatorio.pdf\"\n",
    "pdf_text = read_pdf(file_path)\n",
    "\n",
    "chunks = create_chunks(pdf_text)\n",
    "\n",
    "db = create_vector_store(chunks, OPENAI_API_KEY)\n",
    "\n",
    "user_query = \"Qual foi o tipo de técnica utilizada para a otimização dos hiperparâmetros?\"\n",
    "\n",
    "resposta = rag_pipeline(db, user_query, OPENAI_API_KEY)\n",
    "    \n",
    "print(resposta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

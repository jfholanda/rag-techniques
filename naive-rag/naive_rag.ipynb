{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_and_create_chunks(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lê o conteúdo de um arquivo PDF ou CSV e divide o texto em chunks (partes menores) com sobreposição para preservar o contexto\n",
    "    \n",
    "    Args:\n",
    "        file_path: Caminho para o arquivo PDF ou CSV.\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: Lista de chunks representados como documentos.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    file_extension = file_path.split(\".\")[-1]\n",
    "\n",
    "    if file_extension == \"pdf\":\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_extension == \"csv\":\n",
    "        loader = CSVLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de loader não suportado: {file_extension}. Use 'pdf' ou 'csv'.\")\n",
    "\n",
    "    return loader.load_and_split(text_splitter=text_splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks: List[Document], openai_api_key: str) -> Chroma:\n",
    "    \"\"\"\n",
    "    Cria um banco de vetores para armazenar os embeddings gerados a partir dos chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Lista de chunks do texto.\n",
    "        openai_api_key: Chave da API da OpenAI para geração de embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        Chroma: Banco de dados vetorial criado.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"db\")\n",
    "    print(\"Banco de vetores criado com sucesso.\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt() -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Configura o prompt para geração de respostas baseadas no contexto recuperado.\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: Template de prompt configurado.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Answer the user query based on context. If you don't know the answer \n",
    "    or the context does not have the answer, say that you don't know.\n",
    "    ALWAYS answer in pt-BR. \n",
    "    Provide the source of your knowledge. I want the answer with a link or document reference.\n",
    "\n",
    "    ## CONTEXT\n",
    "    {contexto}\n",
    "\n",
    "    ## USER QUERY\n",
    "    {pergunta}\n",
    "    \"\"\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(db: Chroma, user_query: str, openai_api_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Pipeline de RAG que recupera informações relevantes e gera uma resposta.\n",
    "    \n",
    "    Args:\n",
    "        db: Banco de vetores criado.\n",
    "        user_query: Pergunta do usuário.\n",
    "        openai_api_key: Chave da API da OpenAI para geração de respostas.\n",
    "    \n",
    "    Returns:\n",
    "        str: Resposta gerada com base no contexto recuperado.\n",
    "    \"\"\"\n",
    "    print(\"Buscando informações relevantes no banco de vetores...\")\n",
    "    context = db.similarity_search_with_relevance_scores(user_query, k=3)\n",
    "    context = [c for c, score in context if score >= 0.7]\n",
    "\n",
    "    if not context:\n",
    "        return \"Eu não sou capaz de responder a essa pergunta.\"\n",
    "\n",
    "    context_text = \"\\n\\n\".join([f\"## Documento {i+1}\\n{chunk.page_content}\" for i, chunk in enumerate(context)])\n",
    "    \n",
    "    print(\"Gerando resposta com base no contexto...\\n\")\n",
    "    prompt = setup_prompt()\n",
    "    chain = (prompt |\n",
    "             ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=openai_api_key) |\n",
    "             StrOutputParser())\n",
    "    \n",
    "    print(\"Resposta:\\n\")\n",
    "    \n",
    "    return chain.invoke({\"contexto\": context_text, \"pergunta\": user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco de vetores criado com sucesso.\n",
      "\n",
      "Buscando informações relevantes no banco de vetores...\n",
      "Gerando resposta com base no contexto...\n",
      "\n",
      "Resposta:\n",
      "\n",
      "O imóvel que tem vista para a praia é o apartamento localizado na Av. Praia de Ponta Negra, 1001. Ele possui 2 quartos, 2 banheiros, 1 vaga e está à venda por R$ 850.000. A descrição menciona que tem vista para o mar, sacada e acabamento premium.\n",
      "\n",
      "Fonte: Documento 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "file_path = \"./imoveis.csv\"\n",
    "\n",
    "chunks = read_file_and_create_chunks(file_path)\n",
    "\n",
    "db = create_vector_store(chunks, OPENAI_API_KEY)\n",
    "\n",
    "user_query = \"Qual imóvel tem vista para praia?\"\n",
    "\n",
    "resposta = rag_pipeline(db, user_query, OPENAI_API_KEY)\n",
    "    \n",
    "print(resposta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
